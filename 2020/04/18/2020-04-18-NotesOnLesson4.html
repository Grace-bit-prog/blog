<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-18-NotesOnLesson4.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><b>Tensors</b></p>
<p><u>Tensors</u> are like arrays. Pytorch tensors are better for deep learning than numpy arrays because they are faster and support GPUs, and they support calculating gradients.</p>
<p><u>Rank</u> is the number of axes or dimensions in a tensor.  Types of tensors:</p>

<pre><code>- rank zero: scalar
- rank one: vector
- rank two: matrix


</code></pre>
<p><u>Shape</u> is the size of each axis of a tensor.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]]</span>
<span class="n">arr</span> <span class="o">=</span> <span class="n">array</span> <span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">tns</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tns</span>  <span class="c1"># pytorch</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[1, 2, 3],
        [4, 5, 6]])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tns</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># select a row of the tensor array</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([1, 2, 3])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tns</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># select a column, or dimension of the tensor array</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([3, 6])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tns</span><span class="o">+</span><span class="mi">5</span> <span class="c1"># you can perform computations easily on tensors</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 6,  7,  8],
        [ 9, 10, 11]])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We need to see how good our model is, so we calulate the metric using a validation data set.  We can do this by writing a function that calulates the error rate by computing the mean distance between the valid images and our image that we're testing.</p>
<p>Broadcasting is a feature where PyTorch will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Stochastic-Gradient-Descent-(SGD)">Stochastic Gradient Descent (SGD)<a class="anchor-link" href="#Stochastic-Gradient-Descent-(SGD)"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are the 7 steps of SGD to turn this function into a machine learning classifier:</p>
<ol>
<li><strong>Initialize</strong> the weights: we need to initialize to random values</li>
<li>For each image, use these weights to predict whether it appears to be a three or a seven</li>
<li><strong>Loss</strong>: Based on these predictions, calculate how good the model is (its loss).  We can see if we need to adjust the weights of a model.</li>
<li>Calculate the <strong>Gradient</strong>, which measures for each weight, how changing that weight would change the loss.<br />
The <strong>gradient</strong> only tells us the <strong>slope</strong> of our function, it doesn't actually tell us exactly how far to adjust the parameters. But it gives us some idea of how far; if the slope is very large, then that may suggest that we have more adjustments to do, whereas if the slope is very small, that may suggest that we are close to the optimal value.</li>
<li><p><strong>Step</strong> (that is, change) all weights based on that calculation.  You can try increasing and decreasing the weights to see the results.  The <strong>learning rate (LR)</strong> is multiplying the gradient by a small number, and use optimization. We can use mini batches, or batch size to do step optimization so that it's more efficient and doesn't take as long.  We can use a <code>DataLoader</code> can take any Python collection, and turn it into an iterator over many batches.</p>
<p>w -= gradient(w) * lr</p>
</li>
</ol>
<ol>
<li>Go back to the second step, and repeat the process</li>
<li>...until you decide to <strong>stop</strong> the training process (for instance because the model is good enough, or you don't want to wait any longer).  You would typically stop when the accuracy of the model starts to get worse.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Sigmoid</strong> is a function that is used to make sure values of 0 to 1 are returned when calculating the gradient.</p>
<p><strong>Remember</strong></p>
<ul>
<li><em>activations</em>: numbers that are calculated (both by linear and non-linear layers)</li>
<li><em>parameters</em>: numbers that are randomly initialised, and optimised (that is, the numbers that define the model)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">dl</span><span class="p">:</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">parameters</span> <span class="o">-=</span> <span class="n">parameters</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Optimizing-the-Model">Optimizing the Model<a class="anchor-link" href="#Optimizing-the-Model"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><p>Create a function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters</p>
</li>
<li><p>Find the best set of parameters for any function (stochastic gradient descent)</p>
</li>
<li><p><code>learner.fit</code>:  We can create a <strong>learner</strong> by passing it in the data loader collection along with all the validation data set.</p>

<pre><code>  learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,
  loss_func=mnist_loss, metrics=batch_accuracy)</code></pre>
</li>
</ul>

</div>
</div>
</div>
</div>

