{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Lesson 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tensors</b>\n",
    "\n",
    "  <u>Tensors</u> are like arrays. Pytorch tensors are better for deep learning than numpy arrays because they are faster and support GPUs, and they support calculating gradients.\n",
    "  \n",
    "  \n",
    "  <u>Rank</u> is the number of axes or dimensions in a tensor.  Types of tensors:\n",
    "  \n",
    "    - rank zero: scalar\n",
    "    - rank one: vector\n",
    "    - rank two: matrix\n",
    "\n",
    "  \n",
    "  <u>Shape</u> is the size of each axis of a tensor.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai2.vision.all import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1,2,3],[4,5,6]]\n",
    "arr = array (data)\n",
    "tns = tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns  # pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns[0] # select a row of the tensor array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns[:,2] # select a column, or dimension of the tensor array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns+5 # you can perform computations easily on tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to see how good our model is, so we calulate the metric using a validation data set.  We can do this by writing a function that calulates the error rate by computing the mean distance between the valid images and our image that we're testing.\n",
    "\n",
    "Broadcasting is a feature where PyTorch will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the 7 steps of SGD to turn this function into a machine learning classifier:\n",
    "\n",
    "1.  **Initialize** the weights: we need to initialize to random values\n",
    "2.  For each image, use these weights to predict whether it appears to be a three or a seven\n",
    "3.  **Loss**: Based on these predictions, calculate how good the model is (its loss).  We can see if we need to adjust the weights of a model.\n",
    "4.  Calculate the **Gradient**, which measures for each weight, how changing that weight would change the loss.  \n",
    "The **gradient** only tells us the **slope** of our function, it doesn't actually tell us exactly how far to adjust the parameters. But it gives us some idea of how far; if the slope is very large, then that may suggest that we have more adjustments to do, whereas if the slope is very small, that may suggest that we are close to the optimal value.\n",
    "5.  **Step** (that is, change) all weights based on that calculation.  You can try increasing and decreasing the weights to see the results.  The **learning rate (LR)** is multiplying the gradient by a small number, and use optimization. We can use mini batches, or batch size to do step optimization so that it's more efficient and doesn't take as long.  We can use a `DataLoader` can take any Python collection, and turn it into an iterator over many batches.\n",
    "\n",
    "   w -= gradient(w) * lr\n",
    "   \n",
    "\n",
    "6.  Go back to the second step, and repeat the process\n",
    "7.  ...until you decide to **stop** the training process (for instance because the model is good enough, or you don't want to wait any longer).  You would typically stop when the accuracy of the model starts to get worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid** is a function that is used to make sure values of 0 to 1 are returned when calculating the gradient.\n",
    "\n",
    "**Remember**\n",
    "\n",
    "- *activations*: numbers that are calculated (both by linear and non-linear layers)\n",
    "- *parameters*: numbers that are randomly initialised, and optimised (that is, the numbers that define the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for x,y in dl:\n",
    "    pred = model(x)\n",
    "    loss = loss_func(pred, y)\n",
    "    loss.backward()\n",
    "    parameters -= parameters.grad * lr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters\n",
    "\n",
    "- Find the best set of parameters for any function (stochastic gradient descent)\n",
    "\n",
    "- `learner.fit`:  We can create a **learner** by passing it in the data loader collection along with all the validation data set.\n",
    "\n",
    "        learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n",
    "        loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
